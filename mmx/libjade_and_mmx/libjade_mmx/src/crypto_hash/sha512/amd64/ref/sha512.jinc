
require "sha512_globals.jinc"

inline fn __initH_ref() -> #mmx reg u64[8]
{
  inline int i;
  #mmx reg u64[8] H;
  reg ptr u64[8] Hp;
  reg u64 v;

  Hp = SHA512_H;

  for i=0 to 8
  { v = Hp[i];
    H[i] = v; }

  return H;
}

inline fn __load_H_ref(#mmx reg u64[8] H) -> reg u64, reg u64, reg u64, reg u64,
                                             reg u64, reg u64, reg u64, reg u64
{
  reg u64 a b c d e f g h;

  a = H[0];
  b = H[1];
  c = H[2];
  d = H[3];
  e = H[4];
  f = H[5];
  g = H[6];
  h = H[7];

  return a, b, c, d, e, f, g, h;
}

inline fn __store_H_ref(reg u64 a b c d e f g h) -> #mmx reg u64[8]
{
  #mmx reg u64[8] H;

  H[0] = a;
  H[1] = b;
  H[2] = c;
  H[3] = d;
  H[4] = e;
  H[5] = f;
  H[6] = g;
  H[7] = h;

  return H;
}

inline fn __store_ref(reg u64 out, #mmx reg u64[8] H)
{
  inline int i;
  reg u64 v;

  for i=0 to 8
  { v = H[i];
    v = #BSWAP_64(v);
    (u64)[out + i*8] = v;
  }
}

inline fn __SHR_ref(reg u64 x, inline int c) -> reg u64
{
  reg u64 r;
  r   = x;
  r >>= c;
  return r;
}

inline fn __ROTR_ref(reg u64 x, inline int c) -> reg u64
{
  reg u64 r;
  r = x;
  _, _, r = #ROR_64(r, c);
  return r;
}

//(x & y) ^ (!x & z)
inline fn __CH_ref(reg u64 x y z) -> reg u64
{
  reg u64 r s;

  r  =  x;
  r &=  y;
  s  =  x;
  s  = !s;
  s &=  z;
  r ^=  s;

  return r;
}

//(x & y) ^ (x & z) ^ (y & z)
inline fn __MAJ_ref(reg u64 x y z) -> reg u64
{
  reg u64 r s;

  r  = x;
  r &= y;
  s  = x;
  s &= z;
  r ^= s;
  s  = y;
  s &= z;
  r ^= s;

  return r;
}

// (x >>> 28) ^ (x >>> 34) ^ (x >>> 39)
inline fn __BSIG0_ref(reg u64 x) -> reg u64
{
  reg u64 r s;

  r  = __ROTR_ref(x, 28);
  s  = __ROTR_ref(x, 34);
  r ^= s;
  s  = __ROTR_ref(x, 39);
  r ^= s;

  return r;
}

// (x >>> 14) ^ (x >>> 18) ^ (x >>> 41)
inline fn __BSIG1_ref(reg u64 x) -> reg u64
{
  reg u64 r s;

  r  = __ROTR_ref(x, 14);
  s  = __ROTR_ref(x, 18);
  r ^= s;
  s  = __ROTR_ref(x, 41);
  r ^= s;

  return r;
}

// (x >>> 1) ^ (x >>> 8) ^ (x >> 7)
inline fn __SSIG0_ref(reg u64 x) -> reg u64
{
  reg u64 r s;

  r  = __ROTR_ref(x, 1);
  s  = __ROTR_ref(x, 8);
  r ^= s;
  s  = __SHR_ref(x, 7);
  r ^= s;

  return r;
}

// (x >>> 19) ^ (x >>> 61) ^ (x >> 6)
inline fn __SSIG1_ref(reg u64 x) -> reg u64
{
  reg u64 r s;

  r  = __ROTR_ref(x, 19);
  s  = __ROTR_ref(x, 61);
  r ^= s;
  s  = __SHR_ref(x, 6);
  r ^= s;

  return r;
}

// Wt = SSIG1(W(t-2)) + W(t-7) + SSIG0(t-15) + W(t-16)
inline fn __Wt_ref(stack u64[80] W, inline int t) -> stack u64[80]
{
  reg u64 wt wt2 wt15;

  wt2  = W[t-2];
  wt   = __SSIG1_ref(wt2);
  wt  += W[t-7];
  wt15 = W[t-15];
  wt15 = __SSIG0_ref(wt15);
  wt  += wt15;
  wt  += W[t-16];

  W[t] = wt;

  return W;
}

fn _blocks_0_ref(#mmx reg u64[8] H, reg u64 in inlen) -> #mmx reg u64[8], reg u64, reg u64
{
  inline int t;
  reg u64 T1 T2 a b c d e f g h r v;
  reg u64[8] Hr;
  stack u64[80] W;
  reg ptr u64[80] Kp;
  reg u64 tr;
  stack u64 in_s;

  Kp = SHA512_K;

  while(inlen >= 128)
  {
    for t=0 to 16
    { v = (u64)[in + t*8];
      v = #BSWAP_64(v);
      W[t] = v;
    }
    in_s = in;

    for t=16 to 80
    { W = __Wt_ref(W, t); }

    a, b, c, d, e, f, g, h = __load_H_ref(H);

    tr = 0;
    while(tr < 80)
    {
      //T1 = h + BSIG1(e) + CH(e,f,g) + Kt + Wt
      T1  = h;
      r   = __BSIG1_ref(e);
      T1 += r;
      r   = __CH_ref(e,f,g);
      T1 += r;
      T1 += Kp[(int)tr];
      T1 += W[(int)tr];

      //T2 = BSIG0(a) + MAJ(a,b,c)
      T2  = __BSIG0_ref(a);
      r   = __MAJ_ref(a,b,c);
      T2 += r;

      h  = g;
      g  = f;
      f  = e;
      e  = d;
      e += T1;
      d  = c;
      c  = b;
      b  = a;
      a  = T1;
      a += T2;

      tr+= 1;
    }

    // seems to be 700cycles slower on dev machine for 16KiB // confirm on bench machine // test with "sliding window" to see if it makes a difference
    //                Hr[0] = H[0];
    //                Hr[1] = H[1];
    //                Hr[2] = H[2];
    //                Hr[3] = H[3];
    //    a += Hr[0]; Hr[4] = H[4];
    //    b += Hr[1]; Hr[5] = H[5];
    //    c += Hr[2]; Hr[6] = H[6];
    //    d += Hr[3]; Hr[7] = H[7];
    //    e += Hr[4];
    //    f += Hr[5];
    //    g += Hr[6];
    //    h += Hr[7];

    // 
            v = H[0];
    a += v; v = H[1];
    b += v; v = H[2];
    c += v; v = H[3];
    d += v; v = H[4];
    e += v; v = H[5];
    f += v; v = H[6];
    g += v; v = H[7];
    h += v;


    H = __store_H_ref(a,b,c,d,e,f,g,h);
    //Hp = H;

    in = in_s;
    in += 128;
    inlen -= 128;
  }

  return H, in, inlen;
}

fn _blocks_1_ref(#mmx reg u64[8] H, reg ptr u64[32] sblocks, reg u64 nblocks) -> #mmx reg u64[8], reg ptr u64[32]
{
  inline int t;
  reg u64 T1 T2 a b c d e f g h r v;
  reg u64[8] Hr;
  stack u64[80] W;
  reg ptr u64[80] Kp;
  stack ptr u64[32] s_sblocks;
  reg u64 i oblocks tr;
  stack u64 s_i;

  Kp = SHA512_K;
  i = 0;

  while(i < nblocks)
  {
    s_i = i;
    oblocks = i << 4;
    for t=0 to 16
    { v = sblocks[(int)oblocks + t];
      v = #BSWAP_64(v);
      W[t] = v;
    }
    s_sblocks = sblocks;

    for t=16 to 80
    { W = __Wt_ref(W, t); }

    a, b, c, d, e, f, g, h = __load_H_ref(H);

    tr = 0;
    while(tr < 80)
    {
      //T1 = h + BSIG1(e) + CH(e,f,g) + Kt + Wt
      T1  = h;
      r   = __BSIG1_ref(e);
      T1 += r;
      r   = __CH_ref(e,f,g);
      T1 += r;
      T1 += Kp[(int)tr];
      T1 += W[(int)tr];

      //T2 = BSIG0(a) + MAJ(a,b,c)
      T2  = __BSIG0_ref(a);
      r   = __MAJ_ref(a,b,c);
      T2 += r;

      h  = g;
      g  = f;
      f  = e;
      e  = d;
      e += T1;
      d  = c;
      c  = b;
      b  = a;
      a  = T1;
      a += T2;

      tr+= 1;
    }

    // check corresponding notes: blocks_0
    // 
            v = H[0];
    a += v; v = H[1];
    b += v; v = H[2];
    c += v; v = H[3];
    d += v; v = H[4];
    e += v; v = H[5];
    f += v; v = H[6];
    g += v; v = H[7];
    h += v;

    H = __store_H_ref(a,b,c,d,e,f,g,h);

    sblocks = s_sblocks;
    i = s_i;
    i += 1;
  }

  return H, sblocks;
}


inline fn __lastblocks_ref(reg u64 in inlen bits) -> stack u64[32], reg u64
{
  stack u64[32] sblocks;
  inline int k;
  reg u64 i j nblocks;
  reg u8 v;

  i = 0;

  // Zero-fill the sblocks array
  for k = 0 to 32 { sblocks[k] = i; }

  // copy in to sblocks
  while(i < inlen)
  { v = (u8)[in + i];
    sblocks[u8 (int)i] = v;
    i += 1;
  }

  // set first byte after input to 0x80 
  sblocks[u8 (int)i] = 0x80;

  // check if one or two blocks are needed
  if(inlen < 112)
  { j = (128-8); nblocks = 1; i = 127; }
  else
  { j = (256-8); nblocks = 2; i = 255; }

  while(i >= j)
  { sblocks[u8 (int)i] = (8u) bits;
    bits >>= 8;
    i -= 1;
  }

  return sblocks, nblocks;
}

inline fn __sha512_ref(reg u64 out in inlen)
{
  reg u64 bits nblocks;
  stack u64 s_out s_bits;
  #mmx reg u64[8] H;
  stack u64[32] sblocks;
  reg ptr u64[32] sblocksp;

  s_out = out;

  bits = inlen;
  bits <<= 3;
  s_bits = bits;

  H = __initH_ref();
  H, in, inlen = _blocks_0_ref(H, in, inlen);

  bits = s_bits;
  sblocks, nblocks = __lastblocks_ref(in, inlen, bits);
  sblocksp = sblocks;
  
  H, _ = _blocks_1_ref(H, sblocksp, nblocks);
  
  out = s_out;
  __store_ref(out, H);
}


