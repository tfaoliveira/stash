from Stdlib require "comparisons.jinc"
from Stdlib require "swaps.jinc"
from Keccak require "keccakf1600_4x/keccakf1600_4x_compact.jinc"

require "../ref_opt1/gen_matrix_ref.jinc"

require "gen_matrix_avx2_spills.jinc"
require "gen_matrix_avx2_globals.jinc"


// xof related code

inline fn xof_init_x4(
  reg const ptr u8[32] rho,
  reg u16[4] indexes)
  ->
  stack u256[25]
{
  stack u256[25] state;
  inline int i;
  reg u256 t;
  reg u64 r;

  // copy rho sto state
  for i=0 to 4
  { r = rho[u64 i];
    state[u64 4*i + 0] = r;
    state[u64 4*i + 1] = r;
    state[u64 4*i + 2] = r;
    state[u64 4*i + 3] = r;
   }

  // init to zero
  t = #set0_256();
  for i=4 to 25
  { state[i] = t; }

  // complete the state initialization
  for i=0 to 4
  { state[u16 64 + 4*i]  = indexes[i];
    state[u8 130 + 8*i] ^= 0x1F;
    state[u8 (160*4) + 7 + 8*i] ^= 0x80;
  }

  return state;
}


inline fn xof_next_bytes_x3_x4(
  stack u256[25] state)
  ->
  stack u64[((21 * 3) + 4 + 1) * 4],
  stack u256[25]
{
  reg ptr u256[25] state_ptr;
  stack u64[((21 * 3) + 4 + 1) * 4] b168_x4;
  reg ptr u64[(21 * 3) + 4 + 1] b168_0 b168_1 b168_2 b168_3;

  reg u64 n i;
  reg u256 t;
  reg u128 l h;

  b168_0 = b168_x4[((21 * 3) + 4 + 1)*0:((21 * 3) + 4 + 1)];
  b168_1 = b168_x4[((21 * 3) + 4 + 1)*1:((21 * 3) + 4 + 1)];
  b168_2 = b168_x4[((21 * 3) + 4 + 1)*2:((21 * 3) + 4 + 1)];
  b168_3 = b168_x4[((21 * 3) + 4 + 1)*3:((21 * 3) + 4 + 1)];

  n = 0;
  while(n < 3 * 21) // executes 3 times
  {
    state_ptr = state;
    state = _keccakf1600_4x(state_ptr);

    i = 0;
    while(i < 21 * 32) // executes 21 times (168/8)
    {
      t = state.[(int)i];

      l = (128u) t;
      h = #VEXTRACTI128(t, 1);

		  b168_0[(int)n] = #VMOVLPD(l);
		  b168_1[(int)n] = #VMOVHPD(l);
		  b168_2[(int)n] = #VMOVLPD(h);
		  b168_3[(int)n] = #VMOVHPD(h);

      n += 1;
      i += 32;
	  }
  }

  // note: the last 4 elements of all b168_ contain the remaining keccak state;
  //       this saves some copies if the scalar version of next_bytes is needed

  // note: uncomment the following statements if it simplifies the proof /
  //       helps the safety checker

  // n = 3 * 21;
  i = 21 * 32;

  while(i < 25 * 32) // execs 4 times: 21*32, 22*32, 23*32, 24*32
  {
    t = state.[(int)i];

    l = (128u) t;
    h = #VEXTRACTI128(t, 1);

	  b168_0[(int)n] = #VMOVLPD(l);
	  b168_1[(int)n] = #VMOVHPD(l);
	  b168_2[(int)n] = #VMOVLPD(h);
	  b168_3[(int)n] = #VMOVHPD(h);

    n += 1;
    i += 32;
  }

  b168_0[(int)n] = 0;
  b168_1[(int)n] = 0;
  b168_2[(int)n] = 0;
  b168_3[(int)n] = 0;


  b168_x4[((21 * 3) + 4 + 1)*0:((21 * 3) + 4 + 1)] = b168_0;
  b168_x4[((21 * 3) + 4 + 1)*1:((21 * 3) + 4 + 1)] = b168_1;
  b168_x4[((21 * 3) + 4 + 1)*2:((21 * 3) + 4 + 1)] = b168_2;
  b168_x4[((21 * 3) + 4 + 1)*3:((21 * 3) + 4 + 1)] = b168_3;

  return b168_x4, state;
}


// 'sample' implementation from crypto-specs/ml-kem/Sampling.ec

fn gen_matrix_sample_iterate_x3_fast(
  reg u64 j,
  reg ptr u64[21*3] b168x3,
  reg ptr u16[KYBER_K * KYBER_K * KYBER_N] matrix,
  reg u64 matrix_offset)
  ->
  reg u64,
  reg ptr u16[KYBER_K * KYBER_K * KYBER_N],
  reg u64
{
  reg u16 d1 d2;
  reg u64 k;
  reg bool condition_loop;
  reg ptr u64[21] b168;

  // variable declarations for the updated loop (code adapted from hakyber repository)
  reg u256 f0 f1 g0 g1 g2 g3;
  reg u256 bound ones mask idx8;
  reg u128 l h;
  reg u64 t64 t64_1 t64_2 t64_3;
  reg u64 good;
  reg ptr u8[2048] idxp;
  reg u64 matrix_offset_at_entry;

  matrix_offset_at_entry = matrix_offset;

  idxp = sample_indexes_g;
  bound = jqx16[u256 0];
  ones = #VPBROADCAST_32u8(sample_one_g);
  mask = #VPBROADCAST_16u16(sample_mask_g);
  idx8 = sample_idx8_g[u256 0];

  k = 0;
  while(k < 48*8)
  {
    f0 = #VPERMQ(b168x3.[u256 (int)k], 0x94);
    f1 = #VPERMQ(b168x3.[u256 24 + (int)k], 0x94);
    f0 = #VPSHUFB_256(f0, idx8);
    f1 = #VPSHUFB_256(f1, idx8);
    g0 = #VPSRL_16u16(f0, 4);
    g1 = #VPSRL_16u16(f1, 4);
    f0 = #VPBLEND_16u16(f0, g0, 0xAA);
    f1 = #VPBLEND_16u16(f1, g1, 0xAA);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);

    g0 = #VPCMPGT_16u16(bound, f0);
    g1 = #VPCMPGT_16u16(bound, f1);

    g0 = #VPACKSS_16u16(g0, g1);
    good = #VPMOVMSKB_u256u64(g0);

    t64 = good;
    t64 &= 0xFF;
    g0 = (256u) #VMOV(idxp[u64 (int)t64]);

    t64_1 = good;
    t64_1 >>= 16;
    t64_1 &= 0xFF;
    l = #VMOV(idxp[u64 (int)t64_1]);

    t64_2 = good;
    t64_2 >>= 8;
    t64_2 &= 0xFF;
    g1 = (256u) #VMOV(idxp[u64 (int)t64_2]);

    t64_3 = good;
    t64_3 >>= 24;
    t64_3 &= 0xFF;
    h = #VMOV(idxp[u64 (int)t64_3]);

    g0 = #VINSERTI128(g0, l, 1);

    _, _, _, _, _, t64 = #POPCNT_64(t64);
    _, _, _, _, _, t64_1 = #POPCNT_64(t64_1);
    t64 += matrix_offset;

    g1 = #VINSERTI128(g1, h, 1);

    t64_1 += t64;
    _, _, _, _, _, t64_2 = #POPCNT_64(t64_2);
    t64_2 += t64_1;
    _, _, _, _, _, t64_3 = #POPCNT_64(t64_3);
    t64_3 += t64_2;

    g2 = #VPADD_32u8(g0, ones);
    g0 = #VPUNPCKL_32u8(g0, g2);
    g3 = #VPADD_32u8(g1, ones);
    g1 = #VPUNPCKL_32u8(g1, g3);

    f0 = #VPSHUFB_256(f0, g0);
    f1 = #VPSHUFB_256(f1, g1);

    matrix.[u128 2*(int)matrix_offset] = (128u)f0;
    matrix.[u128 2*(int)t64] = #VEXTRACTI128(f0, 1);
    matrix.[u128 2*(int)t64_1] = (128u)f1;
    matrix.[u128 2*(int)t64_2] = #VEXTRACTI128(f1, 1);

    matrix_offset = t64_3;
    k += 48;
  }

  j = matrix_offset;
  j -= matrix_offset_at_entry;

  b168 = b168x3[21*2:21];

  k = 48;

  while { condition_loop = comp_u64_l_int_and_u64_l_int(j, KYBER_N, k, 168); } 
        (condition_loop)
  {
    d1, d2, k = gen_matrix_sample_get_two_words(b168, k);

    // if (d1 < q)
    if (d1 < KYBER_Q)
    { matrix[(int) matrix_offset] = d1;
      matrix_offset += 1;
      j += 1;
    }

    // if ((d2 < q) && (j < 256))
    if(d2 < KYBER_Q)
    { if(j < KYBER_N)
      { matrix[(int) matrix_offset] = d2;
        matrix_offset += 1;
        j += 1;
      }
    }
  }

  return j, matrix, matrix_offset;
}

inline fn gen_matrix_sample_x4(
  reg ptr u16[KYBER_K * KYBER_K * KYBER_N] matrix,
  reg u64 matrix_offset,
  stack u256[25] xof_state_x4)
  ->
  reg ptr u16[KYBER_K * KYBER_K * KYBER_N],
  reg u64
{
  // local variables
  stack u64[((21 * 3) + 4 + 1) * 4] b168x3x4; // the +1 is for u256 alignment
  reg ptr u64[21 * 3] b168x3;
  inline int p;
  reg u64 j;
  reg ptr u64[25] xof_state;

  b168x3x4, xof_state_x4 = xof_next_bytes_x3_x4(xof_state_x4);

  for p=0 to 4 // #newfeature (to swap for by while)
  {
    j = 0;
    b168x3 = b168x3x4[((21 * 3) + 4 + 1) * p : 21 * 3];

    j, matrix, matrix_offset = gen_matrix_sample_iterate_x3_fast(j, b168x3, matrix, matrix_offset);

    xof_state = b168x3x4[((21 * 3) + 4 + 1) * (p+1) - (25 + 1) : 25];

    matrix, matrix_offset, _ = gen_matrix_sample_finish(j, matrix, matrix_offset, xof_state);
  }

  return matrix, matrix_offset;
}

u16[2*4*2] gen_matrix_indexes =
{
  0x0000, 0x0001, 0x0002, 0x0100,
  0x0101, 0x0102, 0x0200, 0x0201,
  0x0000, 0x0100, 0x0200, 0x0001,
  0x0101, 0x0201, 0x0002, 0x0102
};

inline fn gen_matrix_get_indexes(
  reg u64 _b,
  reg u64 _t)
  ->
  reg u16[4]
{
  reg u64 b t;
  reg u16[4] idx;
  reg ptr u16[2*4*2] gmi;

  gmi = gen_matrix_indexes;

  b = _b; b <<= 2; // b * 4
  t = _t; t <<= 3; // t * 8
  b += t;

  idx[0] = gmi[(int) b + 0];
  idx[1] = gmi[(int) b + 1];
  idx[2] = gmi[(int) b + 2];
  idx[3] = gmi[(int) b + 3];

  return idx;
}

inline fn gen_matrix_sample_four_polynomials(
  reg u64 b,
  reg u64 transposed,
  reg ptr u8[32] rho,
  reg ptr u16[KYBER_K * KYBER_K * KYBER_N] matrix,
  reg u64 matrix_offset)
  ->
  reg ptr u16[KYBER_K * KYBER_K * KYBER_N],
  reg u64
{
  reg u16[4] indexes;
  stack u256[25] xof_state_x4;

  indexes = gen_matrix_get_indexes(b, transposed);
  
  xof_state_x4 = xof_init_x4(rho, indexes);

  matrix, matrix_offset = gen_matrix_sample_x4(matrix, matrix_offset, xof_state_x4);

  return matrix, matrix_offset;      
}

fn _gen_matrix_avx2(
  reg ptr u16[KYBER_K * KYBER_K * KYBER_N] matrix,
  reg ptr u8[32] rho,
  reg u64 transposed)
  ->
  reg ptr u16[KYBER_K * KYBER_K * KYBER_N]
{
  // spills
  #mmx reg u64 b_s transposed_s;
  #mmx reg ptr u8[KYBER_SYMBYTES] rho_s;

  // local variables
  reg u64 matrix_offset;
  reg u64 b;
  reg u64 i j;

  matrix_offset = 0;
  b = 0;

  while(b < 2)
  {
    b_s, transposed_s, rho_s = spill_gen_matrix_1_avx2(b, transposed, rho);

    matrix, matrix_offset = gen_matrix_sample_four_polynomials(b, transposed, rho, matrix, matrix_offset);

    b, transposed, rho = unspill_gen_matrix_1_avx2(b_s, transposed_s, rho_s);

    b += 1;
  }

  // sample the last one, (2,2), using scalar code
  i = 2;
  j = 2;
  matrix, matrix_offset = gen_matrix_sample_one_polynomial(i, j, transposed, rho, matrix, matrix_offset);

  return matrix;
}

